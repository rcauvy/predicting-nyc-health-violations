{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cities across the United States are capitalizing on big data. Predictive policing is becoming a prominent tool for public safety in many cities. In Boston, an algorithm helps determine “problem properties” where the city can target interventions. In Chicago, they are protecting citizens by predicting which landlords are not complying with city ordinances. In New York, the Fire Department sends inspectors to the highest risk buildings so they can prevent deadly fires from breaking out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the CDC, more than 48 million Americans per year become sick from food, and an estimated 75% of the outbreaks came from food prepared by caterers, delis, and restaurants. In most cities, health inspections are generally random, which can increase time spent on spot checks at clean restaurants that have been following the rules closely — and missed opportunities to improve health and hygiene at places with more pressing food safety issues.\n",
    "\n",
    "The goal for this project is to leverage public citizen generated data from social media to narrow the search for critical health and safety violations in New York City. As the City of New York manages  an open data portal, everyone can access historical hygiene inspections and violation records. By combine these two data source this project aims to determine which words, phrases, ratings, and patterns among restaurants lead to critical health and safety violations. This model can assist city health inspectors do their job better by prioritizing the kitchens most likely to be in violation of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The New York Health Department inspects the approximately 27,000 restaurants within the city to monitor their compliance with food safety regulations. Inspectors observe how food is prepared, served and stored and whether restaurant workers are practicing good hygiene. They check food temperatures, equipment maintenance and pest control measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T22:41:24.643355Z",
     "start_time": "2022-05-03T22:41:24.569009Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to import required dependencies:\nnumpy: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.8 from \"/opt/anaconda3/envs/learn-env/bin/python\"\n  * The NumPy version is: \"1.19.1\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so, 0x0002): Library not loaded: @rpath/libopenblas.dylib\n  Referenced from: /opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so\n  Reason: tried: '/opt/anaconda3/envs/learn-env/lib/libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/lib/libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/lib/libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/../../../../libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/lib/libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/lib/libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/lib/libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/../../../../libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/lib/libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/lib/libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/bin/../lib/libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/lib/libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/lib/libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/bin/../lib/libopenblas.dylib' (no such file), '/usr/local/lib/libopenblas.dylib' (no such file), '/usr/lib/libopenblas.dylib' (no such file)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5e23f98c61c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmissing_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     raise ImportError(\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;34m\"Unable to import required dependencies:\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_dependencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to import required dependencies:\nnumpy: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.8 from \"/opt/anaconda3/envs/learn-env/bin/python\"\n  * The NumPy version is: \"1.19.1\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so, 0x0002): Library not loaded: @rpath/libopenblas.dylib\n  Referenced from: /opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so\n  Reason: tried: '/opt/anaconda3/envs/learn-env/lib/libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/lib/libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/lib/libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/../../../../libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/lib/libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/lib/libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/lib/libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/../../../../libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/lib/libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/lib/libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/bin/../lib/libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/lib/libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/lib/libopenblas.dylib' (no such file), '/opt/anaconda3/envs/learn-env/bin/../lib/libopenblas.dylib' (no such file), '/usr/local/lib/libopenblas.dylib' (no such file), '/usr/lib/libopenblas.dylib' (no such file)\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import plotly.express as px\n",
    "\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "import folium\n",
    "import folium.plugins as plugins\n",
    "\n",
    "# misc\n",
    "import glob, os\n",
    "import ast\n",
    "\n",
    "call_apis = False\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_colwidth',200)\n",
    "pd.set_option('display.max_columns',50)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you predict if a restaurant has received a C Grade?\n",
    "### Can you predict a restaurants yelp rating based on its review text?\n",
    "### Can you recommend a similar restaurant nearby with a better health score for restaurants searched for with a C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project there will be two sources and types of data used:\n",
    "\n",
    "* Historical health and hygiene inspections recorded by New York City Department of Health and Mental Hygiene (DOHMH) public health inspectors\n",
    "* User generated Yelp business ratings and reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project requires data pulled from two different sources, the City of New York and Yelp. To obtain the data we will call the API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > The dataset contains every sustained or not yet adjudicated violation citation from every full or special program inspection conducted up to three years prior to the most recent inspection for restaurants and college cafeterias in an active status on the RECORD DATE (date of the data pull). When an inspection results in more than one violation, values for associated fields are repeated for each additional violation record. Establishments are uniquely identified by their CAMIS (record ID) number. Keep in mind that thousands of restaurants start business and go out of business every year; only restaurants in an active status are included in the dataset.\n",
    "Records are also included for each restaurant that has applied for a permit but has not yet been inspected and for inspections resulting in no violations. Establishments with inspection date of 1/1/1900 are new establishments that have not yet received an inspection. Restaurants that received no violations are represented by a single row and coded as having no violations using the ACTION field.\n",
    "Because this dataset is compiled from several large administrative data systems, it contains some illogical values that could be a result of data entry or transfer errors. Data may also be missing.\n",
    "This dataset and the information on the Health Department’s Restaurant Grading website come from the same data source. The Health Department’s Restaurant Grading website is here:\n",
    "http://www1.nyc.gov/site/doh/services/restaurant-grades.page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Why does the Health Department inspect restaurants?\n",
    "The Health Department inspects the approximately 27,000 restaurants in New York City to monitor their compliance with food safety regulations. Inspectors observe how food is prepared, served and stored and whether restaurant workers are practicing good hygiene. They check food temperatures, equipment maintenance and pest control measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since 2010, New York City has required restaurants to post letter grades that correspond to scores received from sanitary inspections. An inspection score of 0 to 13 is an A, 14 to 27 points is a B, and 28 or more points is a C. Grade cards must be posted where they can easily be seen by people passing by."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The New York City Health Department inspects all food service establishments to make sure they meet Health Code requirements, which helps prevent\n",
    "foodborne illness. How often a restaurant is inspected depends on its inspection score. Restaurants that receive a low score on the initial or first inspection\n",
    "in the inspection cycle are inspected less often than those that receive a high score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The points for a particular violation depend on the health risk it poses to the public. Violations fall into three categories:\n",
    ">* A public health hazard, such as failing to keep food at the right temperature, triggers a minimum of 7 points. If the violation can’t be corrected before the inspection ends, the Health Department may close the restaurant until it’s fixed.\n",
    ">* A critical violation, for example, serving raw food such as a salad without properly washing it first, carries a minimum of 5 points.\n",
    ">* A general violation, such as not properly sanitizing\n",
    "cooking utensils, receives at least 2 points.\n",
    "\n",
    ">Inspectors assign additional points to reflect the extent of the\n",
    "violation. A violation’s condition level can range from 1 (least\n",
    "extensive) to 5 (most extensive). For example, the presence of\n",
    "one contaminated food item is a condition level 1 violation,\n",
    "generating 7 points. Four or more contaminated food items\n",
    "is a condition level 4 violation, resulting in 10 points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How are restaurants graded?\n",
    "Violations found during inspections carry point values, and a restaurant’s score corresponds to a letter grade. The point/grade cut-offs are the same as for mobile food vending letter grading, with fewer points corresponding to a better grade:\n",
    "\n",
    ">* \"A\" grade: 0 to 13 points for sanitary violations\n",
    ">* \"B\" grade: 14 to 27 points for sanitary violations\n",
    ">* \"C\" grade: 28 or more points for sanitary violations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The City of New York inspects all restaurants cyclically. And if a business does not pass it's initial inspection for the cycle, it will be re-inspected in 3-5 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T20:42:31.178897Z",
     "start_time": "2022-04-26T20:41:51.041196Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T20:49:25.366492Z",
     "start_time": "2022-04-26T20:49:22.875460Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T20:54:36.780702Z",
     "start_time": "2022-04-26T20:54:36.595355Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(u'Apple is going to build a U.K. factory for $6 million.')\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 110})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T20:57:32.202557Z",
     "start_time": "2022-04-26T20:57:32.168660Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(u'Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million.')\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Restaurant Inspection Results from NYC Open Data Portal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T15:04:10.996002Z",
     "start_time": "2022-04-19T15:04:10.983796Z"
    }
   },
   "source": [
    "The dataset can be obtained here\n",
    "\n",
    "https://data.cityofnewyork.us/Health/DOHMH-New-York-City-Restaurant-Inspection-Results/43nn-pn8j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was downloaded and saved to this repository. Let's load it in and explore its contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed descriptions about each column can be found in the Restaurant Inspection Data Dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:27:58.424539Z",
     "start_time": "2022-04-20T20:27:56.722397Z"
    }
   },
   "outputs": [],
   "source": [
    "doh_df = pd.read_csv('data/nyc_open_data/DOHMH_New_York_City_Restaurant_Inspection_Results.csv')\n",
    "doh_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 186,227 inspection results. However, when an inspection results in more than one violation, values for associated fields are repeated for each additional violation record. So let's check how many individual restaurants are in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding NYC DOHMH Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:00.049387Z",
     "start_time": "2022-04-20T20:28:00.017190Z"
    }
   },
   "outputs": [],
   "source": [
    "# How many unique restaurants are in this dataset?\n",
    "n_unique = doh_df['CAMIS'].nunique()\n",
    "print(f'There are {n_unique} unique restaurants in the dataset. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:11.402977Z",
     "start_time": "2022-04-20T20:28:11.193390Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get more information about the dataset contents\n",
    "doh_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:01.315256Z",
     "start_time": "2022-04-20T20:28:01.006601Z"
    }
   },
   "outputs": [],
   "source": [
    "doh_df['ZIPCODE'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:16.049718Z",
     "start_time": "2022-04-20T20:28:15.884829Z"
    }
   },
   "outputs": [],
   "source": [
    "doh_df['Community Board'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:02.039708Z",
     "start_time": "2022-04-20T20:28:01.864463Z"
    }
   },
   "outputs": [],
   "source": [
    "doh_df['Council District'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:07.743273Z",
     "start_time": "2022-04-20T20:28:07.574324Z"
    }
   },
   "outputs": [],
   "source": [
    "doh_df['Census Tract'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every row has a restaurant ID, address, date, and score. Let's ensure there aren't any duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:19.252663Z",
     "start_time": "2022-04-20T20:28:18.708945Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'There are {doh_df.duplicated(keep=False).sum()} duplicated rows. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:04.024843Z",
     "start_time": "2022-04-20T20:28:03.340870Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's drop these duplucated rows\n",
    "doh_df.drop_duplicates(keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:22.014263Z",
     "start_time": "2022-04-20T20:28:22.005175Z"
    }
   },
   "outputs": [],
   "source": [
    "# Confirming duplicates have been removed\n",
    "doh_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this project will be leveraging data publicly generated from social media a lookup value will be needed to call the API and join the tables. The Yelp API has an endpoint for Phone Search. This will allow us to pull Yelp business data for each restaurant by proivinging a telephone number. More infomration can be found in the [documentation here.](https://www.yelp.com/developers/documentation/v3/business_search_phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:23.307605Z",
     "start_time": "2022-04-20T20:28:23.287825Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking that every restaunt has a phone number\n",
    "missing_num = doh_df['PHONE'].isna().sum()\n",
    "print(f'There are {missing_num} restaunts missing a telephone number.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:24.737821Z",
     "start_time": "2022-04-20T20:28:24.619602Z"
    }
   },
   "outputs": [],
   "source": [
    "# Since only 13 numbers are missing, these rows can be dropped\n",
    "doh_df.dropna(subset=['PHONE'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:25.273115Z",
     "start_time": "2022-04-20T20:28:25.254818Z"
    }
   },
   "outputs": [],
   "source": [
    "# Confirming records were dropped\n",
    "doh_df['PHONE'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:27.022841Z",
     "start_time": "2022-04-20T20:28:27.011554Z"
    }
   },
   "outputs": [],
   "source": [
    "# How many unique restaurants are remaining?\n",
    "n_unique = doh_df['CAMIS'].nunique()\n",
    "print(f'There are {n_unique} unique restaurants remaining in the dataset. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the date range for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:28.162380Z",
     "start_time": "2022-04-20T20:28:27.944090Z"
    }
   },
   "outputs": [],
   "source": [
    "doh_df['INSPECTION DATE'] =  pd.to_datetime(doh_df['INSPECTION DATE'])\n",
    "begin_date = doh_df['INSPECTION DATE'].min()\n",
    "end_date = doh_df['INSPECTION DATE'].max()\n",
    "print(f'The data ranges from {begin_date} to {end_date}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspections in this dataset range from May 2009 up to March 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable -- NYCDOH Inspection Grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Health code violations found during an inspections carries a point value, and a restaurant’s score corresponds to a letter grade. A lower point score, leads to a better letter grade:\n",
    "\n",
    "* \"A\" grade: 0 to 13 points for sanitary violations\n",
    "* \"B\" grade: 14 to 27 points for sanitary violations\n",
    "* \"C\" grade: 28 or more points for sanitary violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T01:12:45.039186Z",
     "start_time": "2022-04-21T01:12:44.034533Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let see what the score distribution is\n",
    "doh_df['SCORE'].hist(bins=113, figsize=(12,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:31.012825Z",
     "start_time": "2022-04-20T20:28:30.974869Z"
    }
   },
   "outputs": [],
   "source": [
    "doh_df['SCORE'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:32.714018Z",
     "start_time": "2022-04-20T20:28:32.670198Z"
    }
   },
   "outputs": [],
   "source": [
    "doh_graded = doh_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:33.438028Z",
     "start_time": "2022-04-20T20:28:33.377283Z"
    }
   },
   "outputs": [],
   "source": [
    "doh_graded.drop(columns=['INSPECTION DATE', 'ACTION', 'VIOLATION CODE',\n",
    "       'VIOLATION DESCRIPTION', 'CRITICAL FLAG','GRADE',\n",
    "       'GRADE DATE', 'RECORD DATE', 'INSPECTION TYPE'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:34.077700Z",
     "start_time": "2022-04-20T20:28:34.063417Z"
    }
   },
   "outputs": [],
   "source": [
    "doh_graded['A'] = (doh_graded['SCORE'] < 14).astype(int)\n",
    "doh_graded['B'] = (doh_graded['SCORE'] > 13).astype(int) & (doh_df['SCORE'] < 28).astype(int)\n",
    "doh_graded['C'] = (doh_graded['SCORE'] > 27).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:35.390402Z",
     "start_time": "2022-04-20T20:28:34.974123Z"
    }
   },
   "outputs": [],
   "source": [
    "doh_grouped = doh_graded.groupby(by=['CAMIS', 'DBA','CUISINE DESCRIPTION',\n",
    "                                     'BORO', 'BUILDING',\n",
    "                                     'STREET', 'ZIPCODE', 'PHONE', 'Latitude',\n",
    "                                     'Longitude', 'Community Board',\n",
    "                                     'Council District','Census Tract'],dropna=False)['A','B','C'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:35.862171Z",
     "start_time": "2022-04-20T20:28:35.851593Z"
    }
   },
   "outputs": [],
   "source": [
    "(doh_grouped['B'] > 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 19,790 unique restaurants, 9,977 failed an initial cycle inspection at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:37.177464Z",
     "start_time": "2022-04-20T20:28:37.157329Z"
    }
   },
   "outputs": [],
   "source": [
    "(doh_grouped['C'] > 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 19,790 unique restaurants, 5,648 severly failed an initial cycle inspection at least once and are at risk of being closed by the DOHMH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:38.420646Z",
     "start_time": "2022-04-20T20:28:38.408571Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the Target Variable 'Severe' for Restaurants that have scored over 28 points in an initial inspection.\n",
    "doh_grouped['Severe'] = (doh_grouped['C'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T21:44:44.136248Z",
     "start_time": "2022-04-20T21:44:44.012722Z"
    }
   },
   "outputs": [],
   "source": [
    "nyc_df = doh_grouped.reset_index()\n",
    "nyc_df.drop(['A','B','C'],axis=1, inplace=True)\n",
    "nyc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NYC DOH Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:43.680801Z",
     "start_time": "2022-04-20T20:28:43.669764Z"
    }
   },
   "outputs": [],
   "source": [
    "nyc_df['BORO'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:44.111952Z",
     "start_time": "2022-04-20T20:28:44.093970Z"
    }
   },
   "outputs": [],
   "source": [
    "nyc_df['CUISINE DESCRIPTION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:44.821590Z",
     "start_time": "2022-04-20T20:28:44.805180Z"
    }
   },
   "outputs": [],
   "source": [
    "nyc_df['ZIPCODE'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Yelp Buniness and Review Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an understanding of the city's inspection results and have explored that dataset it is time to pull in data from the crowd-sourced review platform Yelp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:50.468194Z",
     "start_time": "2022-04-20T20:28:50.456152Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading in locally stored API credentials. \n",
    "# You can sign up for access and obtain credentials to the Yelp API here: \n",
    "# https://www.yelp.com/developers/documentation/v3\n",
    "\n",
    "with open('/Users/Rob/.secret/yelp_api.json') as f:\n",
    "    creds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:28:50.945276Z",
     "start_time": "2022-04-20T20:28:50.937219Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking creds were properly loaded in\n",
    "creds.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp Business Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T21:46:37.013979Z",
     "start_time": "2022-04-20T21:46:36.995159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Formatting phone numbers provided in the NYCDOH dataset \n",
    "nyc_df['PHONE'] = '+1'+nyc_df['PHONE']\n",
    "\n",
    "# Ensure the list contains unique phone numbers only\n",
    "phone_numbers = set(nyc_df['PHONE'])\n",
    "phone_numbers = list(phone_numbers)\n",
    "number_count = len(phone_numbers)\n",
    "print(f'There are {number_count} unique phone numbers.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we will call the Yelp API at the Phone Search Endpoint for all the numbers in the `phone_numbers` list. However the API only allows 5000 callers per day so we'll slice the list into smaller list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:29:07.339656Z",
     "start_time": "2022-04-20T20:29:07.328446Z"
    }
   },
   "outputs": [],
   "source": [
    "# Slicing the phone list into smaller list to fit under the API daily limit restrictions\n",
    "phone_numbers1 = phone_numbers[1:1000]\n",
    "phone_numbers2 = phone_numbers[1000:2000]\n",
    "phone_numbers3 = phone_numbers[2000:2500]\n",
    "phone_numbers4 = phone_numbers[2500:3500]\n",
    "phone_numbers5 = phone_numbers[3500:5000]\n",
    "phone_numbers6 = phone_numbers[5000:6000]\n",
    "phone_numbers7 = phone_numbers[6000:7500]\n",
    "phone_numbers8 = phone_numbers[7500:10000]\n",
    "phone_numbers9 = phone_numbers[10000:12500]\n",
    "phone_numbers11 = phone_numbers[15000:17500]\n",
    "phone_numbers12 = phone_numbers[17500:20000]\n",
    "phone_numbers9 = phone_numbers[10000:12500]\n",
    "phone_numbers10 = phone_numbers[12500:15000]\n",
    "phone_numbers11 = phone_numbers[15000:17500]\n",
    "phone_numbers12 = phone_numbers[17500:19000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:29:07.906103Z",
     "start_time": "2022-04-20T20:29:07.896533Z"
    }
   },
   "outputs": [],
   "source": [
    "#Functionizing the Yelp API Phone Search\n",
    "\n",
    "def get_businesses(phone_numbers):\n",
    "    \"\"\"Input a list of formatted phone numbers\n",
    "    (must start with + and include the country code, like +14159083801)\n",
    "    and returns a corresponding list of Yelp Businesses\"\"\"\n",
    "    \n",
    "    biz_list = []\n",
    "    \n",
    "    for number in phone_numbers:\n",
    "        url = 'https://api.yelp.com/v3/businesses/search/phone'\n",
    "        headers = {'Authorization': 'Bearer ' + creds['api_key']}\n",
    "        url_params = {'phone': number}\n",
    "        response = requests.get(url, headers=headers, params=url_params)\n",
    "        response_json = response.json()\n",
    "        biz_list.extend(response_json.get('businesses','U'))\n",
    "        \n",
    "    while 'U' in biz_list:\n",
    "        biz_list.remove('U')\n",
    "        \n",
    "    return biz_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:29:08.413023Z",
     "start_time": "2022-04-20T20:29:08.389067Z"
    }
   },
   "outputs": [],
   "source": [
    "# Call `get_business` function\n",
    "if call_apis == True:\n",
    "    biz_list12 = get_businesses(phone_numbers12)\n",
    "    \n",
    "    # Save returned list as a DataFrame and .csv file\n",
    "    biz12_df = pd.DataFrame(biz_list12)\n",
    "    biz12_df.to_csv('data/yelp_data/yelp_business/yelp_phone12.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:29:08.893089Z",
     "start_time": "2022-04-20T20:29:08.878584Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of files containing Yelp business data\n",
    "fpath = 'data/yelp_data/yelp_businesses/'\n",
    "os.listdir(fpath)\n",
    "query = fpath+\"*.csv\"\n",
    "f_list = glob.glob(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:29:20.696802Z",
     "start_time": "2022-04-20T20:29:20.275016Z"
    }
   },
   "outputs": [],
   "source": [
    "# Append saved Yelp Business tables to a dict\n",
    "yelp_tables = {}\n",
    "\n",
    "for f in f_list:\n",
    "    temp_df = pd.read_csv(f)\n",
    "    fname = f.replace('data/yelp_data/yelp_businesses/yelp_phone','df_').replace('.csv','')\n",
    "    yelp_tables[fname] = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:29:21.260456Z",
     "start_time": "2022-04-20T20:29:21.256733Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_df_list = [t for t in list(yelp_tables.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:29:22.116252Z",
     "start_time": "2022-04-20T20:29:22.032148Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenating all Yelp Businesses responses from the Phone Search\n",
    "yelp_businesses_df = pd.concat(yelp_tables,ignore_index=True)\n",
    "yelp_businesses_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring Yelp Businesses Response Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:29:25.405089Z",
     "start_time": "2022-04-20T20:29:25.347277Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_businesses_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:29:26.255587Z",
     "start_time": "2022-04-20T20:29:26.237246Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_businesses_df['price'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:29:26.828857Z",
     "start_time": "2022-04-20T20:29:26.812278Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_businesses_df['rating'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:29:27.648261Z",
     "start_time": "2022-04-20T20:29:27.622623Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_businesses_df['categories'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:29:28.959347Z",
     "start_time": "2022-04-20T20:29:28.928738Z"
    }
   },
   "outputs": [],
   "source": [
    "# Duplicates?\n",
    "yelp_businesses_df[yelp_businesses_df.duplicated(['id'], keep=False)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:29:29.710032Z",
     "start_time": "2022-04-20T20:29:29.669845Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_businesses_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:29:32.813739Z",
     "start_time": "2022-04-20T20:29:32.803732Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_businesses_df['review_count'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the restaurants from the NYC DOHMH dataset have been used to search the Yelp API and have been concatenated we can use the return url to gather reviews for each business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T22:09:37.348828Z",
     "start_time": "2022-04-19T22:09:37.339571Z"
    }
   },
   "outputs": [],
   "source": [
    "# OLD from webscraping\n",
    "# df_10_2 = df_10.loc[1000:2173]\n",
    "# df_10_2.to_csv('df_10_2',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T04:46:22.678589Z",
     "start_time": "2022-04-21T04:46:22.667197Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_text(url_list):\n",
    "    \"\"\" Given a list of urls, this function will iterate through the list and \n",
    "    extract text from the first page of reviews. The data will be joined into \n",
    "    a corpus for each business\"\"\"\n",
    "    \n",
    "    review_txt = []\n",
    "\n",
    "    for url in url_list:\n",
    "        req = requests.get(url,allow_redirects=False)\n",
    "        soup = bs(req.content)\n",
    "        comments = soup.find_all(class_='raw__09f24__T4Ezm', lang=\"en\")\n",
    "        comment_txt = []\n",
    "\n",
    "        for comment in comments:\n",
    "            comment_txt.append(comment.text)\n",
    "\n",
    "        comment_corp = ('.'.join(comment_txt))\n",
    "        review_txt.append(comment_corp)\n",
    "    return review_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the time required to run this function, it can be broken into smaller requests. Since we already have smaller list used when we called the API earlier we can take the urls returned from the API here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:29:36.141494Z",
     "start_time": "2022-04-20T20:29:36.126793Z"
    }
   },
   "outputs": [],
   "source": [
    "# Obtaining list of yelp business urls from saved API response\n",
    "url_list = list(yelp_tables['df_1']['url'])\n",
    "len(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:29:37.023615Z",
     "start_time": "2022-04-20T20:29:37.016083Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calling `get_text` function to obtain Yelp reviews\n",
    "if call_apis == True:\n",
    "    review_text = get_text(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:29:37.609125Z",
     "start_time": "2022-04-20T20:29:37.603003Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the Reviews to a csv in the repository\n",
    "if call_apis == True:\n",
    "    rvw_txt = pd.DataFrame(review_text,columns=['Review_Text'])\n",
    "    rvw_txt.to_csv('rvw_txt1.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this for all the urls returned from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining Yelp Reviews to Yelp Business Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:29:41.221934Z",
     "start_time": "2022-04-20T20:29:41.214428Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of files containing Yelp business data\n",
    "fpath = 'data/yelp_data/yelp_reviews/'\n",
    "os.listdir(fpath)\n",
    "query = fpath+\"*.csv\"\n",
    "f_list = glob.glob(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:29:45.412486Z",
     "start_time": "2022-04-20T20:29:45.400010Z"
    }
   },
   "outputs": [],
   "source": [
    "f_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:29:50.784122Z",
     "start_time": "2022-04-20T20:29:49.708272Z"
    }
   },
   "outputs": [],
   "source": [
    "# Append saved Yelp Reviews to a dict\n",
    "rvw_tables = {}\n",
    "\n",
    "for file in f_list:\n",
    "    temp_df = pd.read_csv(file)\n",
    "    fname = file.replace('data/yelp_data/yelp_reviews/rvw_txt','txt_').replace('.csv','')\n",
    "    rvw_tables[fname] = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:30:12.883494Z",
     "start_time": "2022-04-20T20:30:12.878664Z"
    }
   },
   "outputs": [],
   "source": [
    "rvw_df_list = [t for t in list(rvw_tables.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:30:16.211828Z",
     "start_time": "2022-04-20T20:30:16.144444Z"
    }
   },
   "outputs": [],
   "source": [
    "rvw1 = pd.read_csv('data/yelp_data/yelp_reviews/rvw_txt1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:30:17.299728Z",
     "start_time": "2022-04-20T20:30:17.272005Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_tables.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:30:17.843062Z",
     "start_time": "2022-04-20T20:30:17.831950Z"
    }
   },
   "outputs": [],
   "source": [
    "rvw_tables.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:30:18.614343Z",
     "start_time": "2022-04-20T20:30:18.588764Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adding review text to the Yelp Business tables\n",
    "yelp_tables['df_1']['Reviews'] =  rvw_tables['txt_1']\n",
    "yelp_tables['df_2']['Reviews'] =  rvw_tables['txt_2']\n",
    "yelp_tables['df_3']['Reviews'] =  rvw_tables['txt_3']\n",
    "yelp_tables['df_4']['Reviews'] =  rvw_tables['txt_4']\n",
    "yelp_tables['df_5']['Reviews'] =  rvw_tables['txt_5']\n",
    "yelp_tables['df_6']['Reviews'] =  rvw_tables['txt_6']\n",
    "yelp_tables['df_7']['Reviews'] =  rvw_tables['txt_7']\n",
    "yelp_tables['df_8']['Reviews'] =  rvw_tables['txt_8']\n",
    "yelp_tables['df_9']['Reviews'] =  rvw_tables['txt_9']\n",
    "yelp_tables['df_10']['Reviews'] =  rvw_tables['txt_10']\n",
    "yelp_tables['df_11']['Reviews'] =  rvw_tables['txt_11']\n",
    "yelp_tables['df_12']['Reviews'] =  rvw_tables['txt_12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T21:47:12.785871Z",
     "start_time": "2022-04-20T21:47:12.461694Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenating all Yelp businesses tables with review text included\n",
    "yelp_df = pd.concat(yelp_tables,ignore_index=True)\n",
    "yelp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T21:10:42.746234Z",
     "start_time": "2022-04-06T21:10:42.728669Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOT OLD - clean JSON dataframes\n",
    "\n",
    "# def prepare_data(data_list):\n",
    "#     \"\"\"\n",
    "#     This function takes in a list of dictionaries and prepares it\n",
    "#     for analysis\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Make a new list to hold results\n",
    "#     results = []\n",
    "    \n",
    "#     for business_data in data_list:\n",
    "    \n",
    "#         # Make a new dictionary to hold prepared data for this business\n",
    "#         prepared_data = {}\n",
    "        \n",
    "#         # Extract name, review_count, rating, and price key-value pairs\n",
    "#         # from business_data and add to prepared_data\n",
    "#         # If a key is not present in business_data, add it to prepared_data\n",
    "#         # with an associated value of None\n",
    "#         for key in (\"name\", \"review_count\", \"rating\", \"price\"):\n",
    "#             prepared_data[key] = business_data.get(key, None)\n",
    "    \n",
    "#         # Parse and add latitude and longitude columns\n",
    "#         coordinates = business_data[\"coordinates\"]\n",
    "#         prepared_data[\"latitude\"] = coordinates[\"latitude\"]\n",
    "#         prepared_data[\"longitude\"] = coordinates[\"longitude\"]\n",
    "        \n",
    "#         # Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "#     return results\n",
    "    \n",
    "# # Test out function\n",
    "# prepared_businesses = prepare_data(businesses)\n",
    "# prepared_businesses[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-23T01:52:48.820508Z",
     "start_time": "2022-03-23T01:52:48.809218Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOT OLD - clean JSON dataframes\n",
    "# lat = []\n",
    "# long = []\n",
    "\n",
    "# for _,business in yelp_df.iterrows():\n",
    "#     lat.append(business['coordinates']['latitude'])\n",
    "#     long.append(business['coordinates']['longitude'])\n",
    "\n",
    "# yelp_df['lat'] = lat\n",
    "# yelp_df['long'] = long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T17:22:24.996907Z",
     "start_time": "2022-04-16T17:22:24.990683Z"
    }
   },
   "outputs": [],
   "source": [
    "# OLD Yelp Review API\n",
    "# yelp_ids = list(yelp_businesses_df['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T17:38:23.035631Z",
     "start_time": "2022-04-16T17:38:22.612783Z"
    }
   },
   "outputs": [],
   "source": [
    "# OLD Yelp Review API\n",
    "# review_corp = []\n",
    "\n",
    "# for business in  yelp_ids:\n",
    "#     yelp_id = business\n",
    "#     url = f'https://api.yelp.com/v3/businesses/{yelp_id}/reviews'\n",
    "#     headers = {'Authorization': 'Bearer ' + creds['api_key']}\n",
    "#     response = requests.get(url, headers=headers)\n",
    "#     response_json = response.json()\n",
    "#     response_json\n",
    "    \n",
    "#     for review in response_json.get('reviews'):\n",
    "#         review_corp.append(review['text'])\n",
    "        \n",
    "#     dict_test = {'yelp_id' : yelp_id, 'review_text' : review_corp}\n",
    "\n",
    "#     df_test = pd.DataFrame(dict_test)\n",
    "    \n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T16:05:35.052610Z",
     "start_time": "2022-04-17T16:05:35.026084Z"
    }
   },
   "outputs": [],
   "source": [
    "# OLD Yelp Review API\n",
    "# for review in review_corp:\n",
    "#     print(len(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T02:12:08.605749Z",
     "start_time": "2022-04-15T02:12:08.247867Z"
    }
   },
   "outputs": [],
   "source": [
    "# OLD Yelp Review API\n",
    "# #Review Search \n",
    "# yelp_id = 'fEsgUESZxOQtd4YEmAUuow'\n",
    "# url = f'https://api.yelp.com/v3/businesses/{yelp_id}/reviews'\n",
    "# headers = {'Authorization': 'Bearer ' + creds['api_key']}\n",
    "# response = requests.get(url, headers=headers)\n",
    "# response_json = response.json()\n",
    "# response_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T02:40:22.429405Z",
     "start_time": "2022-04-15T02:40:22.419460Z"
    }
   },
   "outputs": [],
   "source": [
    "# OLD Yelp Review API\n",
    "# review_corp = []\n",
    "\n",
    "# for review in response_json.get('reviews'):\n",
    "#     review_corp.append(review['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T02:44:56.085995Z",
     "start_time": "2022-04-15T02:44:56.082350Z"
    }
   },
   "outputs": [],
   "source": [
    "# OLD Yelp Review API\n",
    "# review_corp = ''.join(review_corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T02:40:24.820929Z",
     "start_time": "2022-04-15T02:40:24.806937Z"
    }
   },
   "outputs": [],
   "source": [
    "# OLD Yelp Review API\n",
    "# dict_test = {'yelp_id' : yelp_id, 'review_text' : review_corp}\n",
    "# df_test = pd.DataFrame(dict_test)\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T15:07:46.486247Z",
     "start_time": "2022-04-14T15:07:46.435062Z"
    }
   },
   "outputs": [],
   "source": [
    "# OLD Yelp Review API\n",
    "# review_df = pd.DataFrame(response_json.get('reviews'))\n",
    "# review_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining NYC DOHMH & Yelp Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T22:10:20.757835Z",
     "start_time": "2022-04-20T22:10:20.733500Z"
    }
   },
   "outputs": [],
   "source": [
    "nyc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:30:29.604315Z",
     "start_time": "2022-04-20T20:30:29.568706Z"
    }
   },
   "outputs": [],
   "source": [
    "nyc_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:30:29.984745Z",
     "start_time": "2022-04-20T20:30:29.942646Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:30:30.737742Z",
     "start_time": "2022-04-20T20:30:30.689675Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T21:47:23.412176Z",
     "start_time": "2022-04-20T21:47:23.393301Z"
    }
   },
   "outputs": [],
   "source": [
    "# Formatting Yelp phone numbers to align with NYC phone numbers to join on\n",
    "yelp_df['phone'] = '+' + yelp_df['phone'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T21:47:24.917433Z",
     "start_time": "2022-04-20T21:47:24.882146Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:30:53.489930Z",
     "start_time": "2022-04-20T20:30:53.480135Z"
    }
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:30:55.927528Z",
     "start_time": "2022-04-20T20:30:55.921439Z"
    }
   },
   "outputs": [],
   "source": [
    "# # must start with 'sqlite:///' for a relative path\n",
    "# engine = create_engine('sqlite:///capstone.db', echo=True) \n",
    "# # echo determines whether actions are output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T20:30:58.121932Z",
     "start_time": "2022-04-20T20:30:58.116957Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Functionize SQL table creation with .to_sql\n",
    "# def create_sql_table(df, table_name, engine):\n",
    "#     df.to_sql(table_name, con=engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T22:08:42.503751Z",
     "start_time": "2022-04-20T22:08:42.493292Z"
    }
   },
   "outputs": [],
   "source": [
    "# create_sql_table(nyc_df, 'nyc_restaurants', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T22:08:37.351244Z",
     "start_time": "2022-04-20T22:08:37.329435Z"
    }
   },
   "outputs": [],
   "source": [
    "# create_sql_table(yelp_df, 'yelp', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T21:16:46.018211Z",
     "start_time": "2022-04-21T21:16:45.713962Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merging NYC and Yelp datasets\n",
    "df_1 = pd.merge(nyc_df, yelp_df, left_on='PHONE', right_on='phone', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T21:16:46.673348Z",
     "start_time": "2022-04-21T21:16:46.539321Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Saving merged dataset to csv\n",
    "# df_1.to_csv('full_dataset.csv')\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T21:16:52.407910Z",
     "start_time": "2022-04-21T21:16:52.284648Z"
    }
   },
   "outputs": [],
   "source": [
    "df_1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrubbing The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T20:26:00.302099Z",
     "start_time": "2022-04-21T20:26:00.295385Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get missing reviews\n",
    "# Drop businesses missing reviews\n",
    "# Drop unneccessary columns\n",
    "# Check duplicates\n",
    "# ??Feature Engineering\n",
    "# Get coordinates\n",
    "# Get takeout/delivery\n",
    "# Get $$$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T21:28:24.940480Z",
     "start_time": "2022-04-21T21:28:24.925760Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting missing reviews\n",
    "missing_reviews = df_1[df_1.Reviews.isna()]\n",
    "urls = missing_reviews['url']\n",
    "len(list(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T07:38:26.193750Z",
     "start_time": "2022-04-21T05:15:32.699566Z"
    }
   },
   "outputs": [],
   "source": [
    "# Re-running function from above with the list of urls missing reviews\n",
    "# adding_reviews = get_text(list(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T21:14:14.683742Z",
     "start_time": "2022-04-21T21:14:14.039815Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the review text as a csv to the repository\n",
    "# mssing_rvws = pd.DataFrame(adding_reviews,columns=['Review_Text'])\n",
    "# mssing_rvws.to_csv('mssing_rvws.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T21:46:26.010225Z",
     "start_time": "2022-04-21T21:46:26.003001Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_reviews2 = missing_reviews.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T21:48:28.942290Z",
     "start_time": "2022-04-21T21:48:28.885352Z"
    }
   },
   "outputs": [],
   "source": [
    "# missing_reviews2.reset_index(inplace=True)\n",
    "missing_reviews2.drop(columns=['level_0'],inplace=True)\n",
    "missing_reviews2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T21:56:32.533144Z",
     "start_time": "2022-04-21T21:56:32.523846Z"
    }
   },
   "outputs": [],
   "source": [
    "urls_df = pd.DataFrame(urls)\n",
    "urls_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T21:57:55.011537Z",
     "start_time": "2022-04-21T21:57:55.004216Z"
    }
   },
   "outputs": [],
   "source": [
    "url_reviews_df = pd.concat([urls_df,adding_reviews],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T21:59:47.446750Z",
     "start_time": "2022-04-21T21:59:47.256724Z"
    }
   },
   "outputs": [],
   "source": [
    "rvw_df = pd.read_csv('mssing_rvws.csv')\n",
    "rvw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T22:02:28.107772Z",
     "start_time": "2022-04-21T22:02:28.042050Z"
    }
   },
   "outputs": [],
   "source": [
    "fill_in_missing_df = pd.concat([missing_reviews2,adding_reviews],axis=1)\n",
    "fill_in_missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T22:10:30.232752Z",
     "start_time": "2022-04-21T22:10:30.228016Z"
    }
   },
   "outputs": [],
   "source": [
    "fill_in_missing_df.rename(columns={'reviews':'Reviews'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T22:07:00.859431Z",
     "start_time": "2022-04-21T22:07:00.850532Z"
    }
   },
   "outputs": [],
   "source": [
    "fill_in_missing_df['reviews'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T22:08:30.341340Z",
     "start_time": "2022-04-21T22:08:30.325215Z"
    }
   },
   "outputs": [],
   "source": [
    "fill_in_missing_df.drop(columns=['Reviews'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T22:10:36.186569Z",
     "start_time": "2022-04-21T22:10:36.101800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Appending businesses with new reviews to full dataset\n",
    "df_2 = pd.concat([df_1,fill_in_missing_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T22:17:57.979541Z",
     "start_time": "2022-04-21T22:17:57.950940Z"
    }
   },
   "outputs": [],
   "source": [
    "# dropping old rows without reviews\n",
    "df_2.dropna(subset=['Reviews'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T22:17:59.141449Z",
     "start_time": "2022-04-21T22:17:59.129876Z"
    }
   },
   "outputs": [],
   "source": [
    "df_2['Reviews'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T22:19:24.938547Z",
     "start_time": "2022-04-21T22:19:24.880105Z"
    }
   },
   "outputs": [],
   "source": [
    "df_2[df_2['Reviews'] == '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still 220 businesses without reviews. Not too bad, but will try to extract these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T22:51:00.293283Z",
     "start_time": "2022-04-21T22:51:00.282908Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_rvws2 = df_2[df_2['Reviews'] == '']['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T22:51:06.478727Z",
     "start_time": "2022-04-21T22:51:06.469893Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_rvws2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T22:30:30.694335Z",
     "start_time": "2022-04-21T22:23:50.086072Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extracting the last reviews\n",
    "# adding_reviews2 = get_text(list(missing_rvws2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T22:33:36.268857Z",
     "start_time": "2022-04-21T22:33:36.234931Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Saving the review text as a csv to the repository\n",
    "# mssing_rvws2 = pd.DataFrame(adding_reviews2,columns=['Review_Text'])\n",
    "# mssing_rvws2.to_csv('mssing_rvws2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T22:52:19.562893Z",
     "start_time": "2022-04-21T22:52:19.556986Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_rvws2_df = pd.DataFrame(missing_rvws2)\n",
    "missing_rvws2_df.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T22:58:40.998609Z",
     "start_time": "2022-04-21T22:58:40.979835Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_rvws2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T22:59:32.379649Z",
     "start_time": "2022-04-21T22:59:32.365728Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T22:52:33.965945Z",
     "start_time": "2022-04-21T22:52:33.946549Z"
    }
   },
   "outputs": [],
   "source": [
    "fill_in_mssing_rvws_df2 = pd.concat([missing_rvws2_df,mssing_rvws2,],axis=1,join='outer',ignore_index=True)\n",
    "fill_in_mssing_rvws_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_in_missing_df2 = pd.concat([missing_reviews2,adding_reviews],axis=1)\n",
    "fill_in_missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T22:56:33.931240Z",
     "start_time": "2022-04-21T22:56:33.921473Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_reviews_2_df = df_2[df_2['Reviews'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:01:01.742101Z",
     "start_time": "2022-04-21T23:01:01.734921Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_reviews_2_df.reset_index(inplace=True)\n",
    "missing_reviews_2_df.drop(columns=['index'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:01:06.196288Z",
     "start_time": "2022-04-21T23:01:06.138164Z"
    }
   },
   "outputs": [],
   "source": [
    "fill_in_missing_df2 = pd.concat([missing_reviews_2_df,mssing_rvws2],axis=1)\n",
    "fill_in_missing_df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:03:01.001559Z",
     "start_time": "2022-04-21T23:03:00.945338Z"
    }
   },
   "outputs": [],
   "source": [
    "fill_in_missing_df2.drop(columns=['Reviews'],inplace=True)\n",
    "fill_in_missing_df2.rename(columns={'Review_Text':'Reviews'},inplace=True)\n",
    "fill_in_missing_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:09:55.573986Z",
     "start_time": "2022-04-21T23:09:55.537965Z"
    }
   },
   "outputs": [],
   "source": [
    "# Appending businesses with newly gathered reviews to full dataset\n",
    "df_3 = pd.concat([df_2,fill_in_missing_df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:09:56.398912Z",
     "start_time": "2022-04-21T23:09:56.342365Z"
    }
   },
   "outputs": [],
   "source": [
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:11:43.594111Z",
     "start_time": "2022-04-21T23:11:43.399508Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropping old rows without reviews\n",
    "df_3 = df_3[df_3.Reviews != '']\n",
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:12:05.736306Z",
     "start_time": "2022-04-21T23:12:03.749592Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for duplicated rows\n",
    "df_3[df_3.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:12:48.244589Z",
     "start_time": "2022-04-21T23:12:47.890855Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropping Duplicated Rows\n",
    "df_3.drop_duplicates(keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:12:50.295534Z",
     "start_time": "2022-04-21T23:12:50.244366Z"
    }
   },
   "outputs": [],
   "source": [
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:15:02.467938Z",
     "start_time": "2022-04-21T23:15:02.446172Z"
    }
   },
   "outputs": [],
   "source": [
    "# Resetting the index\n",
    "df_3.reset_index(inplace=True)\n",
    "df_3.drop(columns='index',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:15:03.465664Z",
     "start_time": "2022-04-21T23:15:03.425512Z"
    }
   },
   "outputs": [],
   "source": [
    "df_3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:16:42.380016Z",
     "start_time": "2022-04-21T23:16:42.366846Z"
    }
   },
   "outputs": [],
   "source": [
    "# Looking at the distributions for each Boro\n",
    "df_3['BORO'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:15:09.545583Z",
     "start_time": "2022-04-21T23:15:09.506080Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inspecting missing Boro's\n",
    "df_3[df_3['BORO'] == '0' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:16:38.839384Z",
     "start_time": "2022-04-21T23:16:38.833419Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imputing missing BORO labels\n",
    "df_3.loc[6780:6781]['BORO'] = 'Brooklyn'\n",
    "df_3.loc[10182:10182]['BORO'] = 'Manhattan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:16:53.713884Z",
     "start_time": "2022-04-21T23:16:53.654803Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looking at Restaurants with missing zipcodes\n",
    "df_3[df_3['ZIPCODE'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:17:40.345464Z",
     "start_time": "2022-04-21T23:17:40.340322Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looking at Restaurants with missing zipcodes\n",
    "missing_zips = df_3['ZIPCODE'].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:17:49.992976Z",
     "start_time": "2022-04-21T23:17:49.096786Z"
    }
   },
   "outputs": [],
   "source": [
    "# Converting location data from str to dict dtype\n",
    "df_3['location'] = df_3['location'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:17:52.289563Z",
     "start_time": "2022-04-21T23:17:52.279235Z"
    }
   },
   "outputs": [],
   "source": [
    "# Confirming change\n",
    "type(df_3.iloc[0]['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:18:06.903780Z",
     "start_time": "2022-04-21T23:18:06.853569Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imputing Zipcodes missing from NYC data with Yelp value\n",
    "addrss_list = list(df_3['location'])\n",
    "zip_list =[]\n",
    "\n",
    "for zipcode in addrss_list:\n",
    "    zip_list.append(zipcode['zip_code'])\n",
    "    \n",
    "df_3['Zip_code'] = zip_list\n",
    "df_3.drop(columns=['ZIPCODE'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:54:11.440525Z",
     "start_time": "2022-04-21T23:54:11.428857Z"
    }
   },
   "outputs": [],
   "source": [
    "df_3['price'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:19:07.784324Z",
     "start_time": "2022-04-21T23:19:07.765838Z"
    }
   },
   "outputs": [],
   "source": [
    "# Converting all review text into string objects\n",
    "df_3['Reviews'].astype(str,errors='raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-22T17:04:53.220629Z",
     "start_time": "2022-04-22T17:04:53.193343Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropping unneccessary columns\n",
    "eda_df = df_3.drop(columns=['display_phone','phone','location','coordinates',\n",
    "                 'categories','is_closed','url','image_url','name','alias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-22T17:04:55.207945Z",
     "start_time": "2022-04-22T17:04:55.187058Z"
    }
   },
   "outputs": [],
   "source": [
    "eda_df['price'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:57:05.825875Z",
     "start_time": "2022-04-21T23:57:05.814878Z"
    }
   },
   "outputs": [],
   "source": [
    "eda_df['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-22T00:01:18.624011Z",
     "start_time": "2022-04-22T00:01:18.306933Z"
    }
   },
   "outputs": [],
   "source": [
    "#Get the distribution of the ratings\n",
    "x=eda_df['rating'].value_counts()\n",
    "x=x.sort_index()\n",
    "#plot\n",
    "plt.figure(figsize=(8,4))\n",
    "ax= sns.barplot(x.index, x.values, alpha=0.8)\n",
    "plt.title(\"Star Rating Distribution\")\n",
    "plt.ylabel('# of businesses', fontsize=12)\n",
    "plt.xlabel('Star Ratings ', fontsize=12)\n",
    "\n",
    "#adding the text labels\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-22T14:53:03.325605Z",
     "start_time": "2022-04-22T14:53:03.297737Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-22T01:46:44.567371Z",
     "start_time": "2022-04-22T01:46:44.555975Z"
    }
   },
   "outputs": [],
   "source": [
    "eda_df['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-22T01:47:12.407128Z",
     "start_time": "2022-04-22T01:47:11.976468Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(eda_df.rating, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:26:23.722856Z",
     "start_time": "2022-04-21T23:26:23.701583Z"
    }
   },
   "outputs": [],
   "source": [
    "cuisines = pd.DataFrame(eda_df['CUISINE DESCRIPTION'].value_counts())\n",
    "cuisines.reset_index(inplace=True)\n",
    "cuisines[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-22T00:08:24.648854Z",
     "start_time": "2022-04-22T00:08:24.118970Z"
    }
   },
   "outputs": [],
   "source": [
    "# What are the popular Cuisine categories?\n",
    "cuisine_cats = eda_df['CUISINE DESCRIPTION']\n",
    "\n",
    "x = cuisine_cats.value_counts()\n",
    "\n",
    "print(\"There are \",len(x),\" different types of cuisines in NYC\")\n",
    "\n",
    "#prep for chart\n",
    "x = x.sort_values(ascending=False)\n",
    "x = x.iloc[0:20]\n",
    "\n",
    "#chart\n",
    "plt.figure(figsize=(16,4))\n",
    "ax = sns.barplot(x.index, x.values, alpha=0.8)#,color=color[5])\n",
    "plt.title(\"What are the top categories?\",fontsize=25)\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=80)\n",
    "plt.ylabel('# businesses', fontsize=12)\n",
    "plt.xlabel('Category', fontsize=12)\n",
    "\n",
    "#adding the text labels\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-22T01:41:27.343646Z",
     "start_time": "2022-04-22T01:41:27.318072Z"
    }
   },
   "outputs": [],
   "source": [
    "eda_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-22T01:43:28.534917Z",
     "start_time": "2022-04-22T01:43:28.497181Z"
    }
   },
   "outputs": [],
   "source": [
    "# Most reviewed restaurants\n",
    "eda_df[['CAMIS','id','DBA', 'review_count', 'rating']].sort_values(ascending=False, by=\"review_count\")[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T16:35:50.058081Z",
     "start_time": "2022-04-20T16:35:50.045598Z"
    }
   },
   "outputs": [],
   "source": [
    "# Look at all the feautres compared to the 0 class and 1 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:26:35.142051Z",
     "start_time": "2022-04-21T23:26:35.128735Z"
    }
   },
   "outputs": [],
   "source": [
    "labels_df = eda_df['Severe']\n",
    "labels_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:26:36.577598Z",
     "start_time": "2022-04-21T23:26:36.315764Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9.2, 5))\n",
    "\n",
    "n_obs = labels_df.shape\n",
    "\n",
    "(clean_df['Severe']\n",
    "    .value_counts()\n",
    "    .plot.barh(title=\"Proportion of Restaurants with Severe Violations\", ax=ax)\n",
    ")\n",
    "ax.set_ylabel(\"Severe Violations\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:26:59.580904Z",
     "start_time": "2022-04-21T23:26:59.556783Z"
    }
   },
   "outputs": [],
   "source": [
    "counts = (eda_df[['BORO', 'Severe']]\n",
    "              .groupby(['BORO', 'Severe'])\n",
    "              .size()\n",
    "              .unstack('Severe')\n",
    "         )\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:27:06.550181Z",
     "start_time": "2022-04-21T23:27:06.202071Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = counts.plot.barh()\n",
    "ax.legend(\n",
    "    loc='center right', \n",
    "    bbox_to_anchor=(1.3, 0.5), \n",
    "    title='Severe Violations'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:27:11.032743Z",
     "start_time": "2022-04-21T23:27:11.023012Z"
    }
   },
   "outputs": [],
   "source": [
    "severe_counts = counts.sum(axis='columns')\n",
    "severe_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:27:18.150623Z",
     "start_time": "2022-04-21T23:27:18.133464Z"
    }
   },
   "outputs": [],
   "source": [
    "props = counts.div(severe_counts, axis='index')\n",
    "props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:29:33.505663Z",
     "start_time": "2022-04-21T23:29:33.198931Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prototyping Stack Barh plot\n",
    "ax = props.plot.barh(stacked=True)\n",
    "ax.set_title('Severe Violations by Boro')\n",
    "ax.set_xlabel('Proportion of Restaurants w/Severe Violations ')\n",
    "ax.legend(\n",
    "    loc='center left', \n",
    "    bbox_to_anchor=(1.05, 0.5),\n",
    "    title='Severe Violations'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:46:20.564282Z",
     "start_time": "2022-04-21T23:46:20.557745Z"
    }
   },
   "outputs": [],
   "source": [
    "def violation_rate_plot(col, target, data, ax=None):\n",
    "    \"\"\"Stacked bar chart of severe health and safety violations rate against \n",
    "    each feature of the data. \n",
    "    \n",
    "    Args:\n",
    "        col (string): column name of feature variable\n",
    "        target (string): column name of target variable\n",
    "        data (pandas DataFrame): dataframe that contains columns \n",
    "            `col` and `target`\n",
    "        ax (matplotlib axes object, optional): matplotlib axes \n",
    "            object to attach plot to\n",
    "    \"\"\"\n",
    "    counts = (eda_df[[target, col]]\n",
    "                  .groupby([target, col])\n",
    "                  .size()\n",
    "                  .unstack(target)\n",
    "             )\n",
    "    group_counts = counts.sum(axis='columns')\n",
    "    props = counts.div(group_counts, axis='index')\n",
    "\n",
    "    props.plot(kind=\"barh\", stacked=True, ax=ax)\n",
    "\n",
    "    ax.legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:31:51.540925Z",
     "start_time": "2022-04-21T23:31:51.533403Z"
    }
   },
   "outputs": [],
   "source": [
    "eda_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T23:46:24.006721Z",
     "start_time": "2022-04-21T23:46:23.100280Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loop through several columns and plot against both h1n1_vaccine and seasonal_vaccine.\n",
    "\n",
    "cols_to_plot = [\n",
    "    'rating',\n",
    "    'transactions',\n",
    "    'price'\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    len(cols_to_plot), figsize=(10,len(cols_to_plot)*2.5)\n",
    ")\n",
    "for idx, col in enumerate(cols_to_plot):\n",
    "    \n",
    "    violation_rate_plot(\n",
    "        col, 'Severe', eda_df, ax=ax[idx]\n",
    "    )\n",
    "    \n",
    "\n",
    "ax[0].legend(\n",
    "    loc='lower center', bbox_to_anchor=(0.5, 1.05), title='Severe Violations'\n",
    ")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-22T17:13:54.773915Z",
     "start_time": "2022-04-22T17:13:54.668606Z"
    }
   },
   "outputs": [],
   "source": [
    "# import street map\n",
    "# https://data.cityofnewyork.us/City-Government/Borough-Boundaries/tqmj-j8zm\n",
    "\n",
    "street_map = gpd.read_file('data/Borough_Boundaries/geo_export_392103e7-13e2-43bf-aaf0-7e5c6a24b7b2.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-22T17:20:48.646727Z",
     "start_time": "2022-04-22T17:20:48.639905Z"
    }
   },
   "outputs": [],
   "source": [
    "geo_eda = eda_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-22T17:20:49.773486Z",
     "start_time": "2022-04-22T17:20:49.757402Z"
    }
   },
   "outputs": [],
   "source": [
    "geo_eda.dropna(axis=0, subset=['Latitude','Longitude'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-22T17:20:53.587106Z",
     "start_time": "2022-04-22T17:20:53.569326Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove the most extreme .1% latitudes, &\n",
    "# the most extreme .1% longitudes\n",
    "\n",
    "geo_eda = geo_eda[\n",
    " (geo_eda['Latitude'] >= np.percentile(geo_eda['Latitude'], 0.05)) & \n",
    " (geo_eda['Latitude'] < np.percentile(geo_eda['Latitude'], 99.95)) &\n",
    " (geo_eda['Longitude'] >= np.percentile(geo_eda['Longitude'], 0.05)) & \n",
    " (geo_eda['Longitude'] <= np.percentile(geo_eda['Longitude'], 99.95))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-22T17:21:18.473190Z",
     "start_time": "2022-04-22T17:21:18.462622Z"
    }
   },
   "outputs": [],
   "source": [
    "geo_eda = geo_eda[geo_eda['Longitude'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-22T17:21:19.356524Z",
     "start_time": "2022-04-22T17:21:19.343406Z"
    }
   },
   "outputs": [],
   "source": [
    "geo_eda = geo_eda[geo_eda['Latitude'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-22T17:21:29.638507Z",
     "start_time": "2022-04-22T17:21:29.627656Z"
    }
   },
   "outputs": [],
   "source": [
    "print(geo_eda['Longitude'].max())\n",
    "print(geo_eda['Longitude'].min())\n",
    "print(geo_eda['Latitude'].max())\n",
    "print(geo_eda['Latitude'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-22T17:21:35.369608Z",
     "start_time": "2022-04-22T17:21:35.187088Z"
    }
   },
   "outputs": [],
   "source": [
    "# designate coordinate system\n",
    "crs = {'init':'epsg:4326'}\n",
    "# zip x and y coordinates into single feature\n",
    "geometry = [Point(xy) for xy in zip(geo_eda['Longitude'], geo_eda['Latitude'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-22T17:21:38.709987Z",
     "start_time": "2022-04-22T17:21:38.696779Z"
    }
   },
   "outputs": [],
   "source": [
    "# create GeoPandas dataframe\n",
    "geo_df = gpd.GeoDataFrame(geo_eda, crs=crs, geometry = geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-23T16:47:09.297777Z",
     "start_time": "2022-04-23T16:47:07.262121Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting map\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "street_map.plot(ax=ax, alpha=.5,color='gray')\n",
    "geo_df[geo_df['Severe'] == 0].plot(ax=ax, alpha=.1, markersize=20,color='blue',label='Pass')\n",
    "geo_df[geo_df['Severe'] == 1].plot(ax=ax, alpha=.1, markersize=20,color='red',label='Fail')\n",
    "plt.legend(prop={'size':15});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all inspections by score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-23T16:48:54.130482Z",
     "start_time": "2022-04-23T16:48:54.126517Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Plot all inspections score or Pass/Fail over time (plotly?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,7))\n",
    "\n",
    "#a random point inside vegas\n",
    "lat = 36.207430\n",
    "lon = -115.268460\n",
    "#some adjustments to get the right pic\n",
    "lon_min, lon_max = lon-0.3,lon+0.5\n",
    "lat_min, lat_max = lat-0.4,lat+0.5\n",
    "#subset for vegas\n",
    "ratings_data_vegas=rating_data[(rating_data[\"longitude\"]>lon_min) &\\\n",
    "                    (rating_data[\"longitude\"]<lon_max) &\\\n",
    "                    (rating_data[\"latitude\"]>lat_min) &\\\n",
    "                    (rating_data[\"latitude\"]<lat_max)]\n",
    "\n",
    "#Facet scatter plot\n",
    "ratings_data_vegas.plot(kind='scatter', x='longitude', y='latitude',\n",
    "                color='yellow', \n",
    "                s=.02, alpha=.6, subplots=True, ax=ax1)\n",
    "ax1.set_title(\"Las Vegas\")\n",
    "ax1.set_facecolor('black')\n",
    "\n",
    "#a random point inside pheonix\n",
    "lat = 33.435463\n",
    "lon = -112.006989\n",
    "#some adjustments to get the right pic\n",
    "lon_min, lon_max = lon-0.3,lon+0.5\n",
    "lat_min, lat_max = lat-0.4,lat+0.5\n",
    "#subset for pheonix\n",
    "ratings_data_pheonix=rating_data[(rating_data[\"longitude\"]>lon_min) &\\\n",
    "                    (rating_data[\"longitude\"]<lon_max) &\\\n",
    "                    (rating_data[\"latitude\"]>lat_min) &\\\n",
    "                    (rating_data[\"latitude\"]<lat_max)]\n",
    "#plot pheonix\n",
    "ratings_data_pheonix.plot(kind='scatter', x='longitude', y='latitude',\n",
    "                color='yellow', \n",
    "                s=.02, alpha=.6, subplots=True, ax=ax2)\n",
    "ax2.set_title(\"Pheonix\")\n",
    "ax2.set_facecolor('black')\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "#rearranging data to suit the format needed for folium\n",
    "stars_list=list(rating_data['stars'].unique())\n",
    "for star in stars_list:\n",
    "    subset=ratings_data_vegas[ratings_data_vegas['stars']==star]\n",
    "    data.append(subset[['latitude','longitude']].values.tolist())\n",
    "#initialize at vegas\n",
    "lat = 36.127430\n",
    "lon = -115.138460\n",
    "zoom_start=11\n",
    "print(\"                     Vegas Review heatmap Animation \")\n",
    "\n",
    "# basic map\n",
    "m = folium.Map(location=[lat, lon], tiles=\"OpenStreetMap\", zoom_start=zoom_start)\n",
    "#inprovising the Heatmapwith time plugin to show variations across star ratings \n",
    "hm = plugins.HeatMapWithTime(data,max_opacity=0.3,auto_play=True,display_index=True,radius=7)\n",
    "hm.add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 8 images\n",
    "imgs_samples = random.sample(train_imgs, 8)\n",
    "\n",
    "# Plot random sample of 8 images\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(len(imgs_samples)):\n",
    "    # OpenCV2 reads images in BGR format\n",
    "    img = cv2.imread(os.path.join(train_dir, imgs_samples[i]))\n",
    "    # Switch color channels to RGB to make compatible with matplotlib imshow func\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # Grab image's business ID and labels\n",
    "    business = train_photo_to_id.loc[train_photo_to_id['photo_id'] == int(imgs_samples[i][:-4]), 'business_id']\n",
    "    labels = train.loc[train['business_id'] == business.values[0], 'labels']\n",
    "    # Annotate each image with image ID, business ID, and labels\n",
    "    title = \"Image ID: \" + imgs_samples[i] + ' Business: ' + str(business.values[0]) + '\\nLabels: ' + ''.join(labels.values)\n",
    "    # Plot the image\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing For Further EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets find most frequent words in Negative reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we will find most frequent words in reviews to get an overview of why users gave low ratings. These words could be related to those business attributes or services about which users are most unhappy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are helper functions \n",
    "# directly copied from https://gist.github.com/benhoyt/dfafeab26d7c02a52ed17b6229f0cb52\n",
    "\n",
    "def tokenize(s):\n",
    "    \"\"\"Convert string to lowercase and split into words (ignoring\n",
    "    punctuation), returning list of words.\n",
    "    \"\"\"\n",
    "    word_list = re.findall(r'\\w+', s.lower())\n",
    "    filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "def count_ngrams(lines, min_length=2, max_length=4):\n",
    "    \"\"\"Iterate through given lines iterator (file object or list of\n",
    "    lines) and return n-gram frequencies. The return value is a dict\n",
    "    mapping the length of the n-gram to a collections.Counter\n",
    "    object of n-gram tuple and number of times that n-gram occurred.\n",
    "    Returned dict includes n-grams of length min_length to max_length.\n",
    "    \"\"\"\n",
    "    lengths = range(min_length, max_length + 1)\n",
    "    ngrams = {length: collections.Counter() for length in lengths}\n",
    "    queue = collections.deque(maxlen=max_length)\n",
    "\n",
    "    # Helper function to add n-grams at start of current queue to dict\n",
    "    def add_queue():\n",
    "        current = tuple(queue)\n",
    "        for length in lengths:\n",
    "            if len(current) >= length:\n",
    "                ngrams[length][current[:length]] += 1\n",
    "\n",
    "    # Loop through all lines and words and add n-grams to dict\n",
    "    for line in lines:\n",
    "        for word in tokenize(line):\n",
    "            queue.append(word)\n",
    "            if len(queue) >= max_length:\n",
    "                add_queue()\n",
    "\n",
    "    # Make sure we get the n-grams at the tail end of the queue\n",
    "    while len(queue) > min_length:\n",
    "        queue.popleft()\n",
    "        add_queue()\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "def print_most_frequent(ngrams, num=10):\n",
    "    \"\"\"Print num most common n-grams of each length in n-grams dict.\"\"\"\n",
    "    for n in sorted(ngrams):\n",
    "        print('----- {} most common {}-word phrase -----'.format(num, n))\n",
    "        for gram, count in ngrams[n].most_common(num):\n",
    "            print('{0}: {1}'.format(' '.join(gram), count))\n",
    "        print('')\n",
    "\n",
    "def print_word_cloud(ngrams, num=5):\n",
    "    \"\"\"Print word cloud image plot \"\"\"\n",
    "    words = []\n",
    "    for n in sorted(ngrams):\n",
    "        for gram, count in ngrams[n].most_common(num):\n",
    "            s = ' '.join(gram)\n",
    "            words.append(s)\n",
    "            \n",
    "    cloud = WordCloud(width=1440, height= 1080,max_words= 200).generate(' '.join(words))\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    plt.imshow(cloud)\n",
    "    plt.axis('off');\n",
    "    plt.show()\n",
    "    print('')2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_business_analysis = 1 # basically this will tell how much computing and diverse our analysis will be\n",
    "business_ids=bottom_business_data.sort_values(\"rated\")[::-1][:num_business_analysis].business_id.values\n",
    "business_names = bottom_business_data.sort_values(\"rated\")[::-1][:num_business_analysis][\"Business name\"].values\n",
    "# get all the reviews and analyse them\n",
    "#business_names\n",
    "for i, business_id in enumerate(business_ids):\n",
    "    # now extract reviews from reviews data\n",
    "    print(\"Analysing business: \",business_names[i])\n",
    "    reviews = yelp_review.loc[yelp_review['business_id'] == business_id].text.values\n",
    "    most_used_text = count_ngrams(reviews,max_length=3)\n",
    "    print_most_frequent(most_used_text, num=10)\n",
    "    print_word_cloud(most_used_text, 10)\n",
    "    #print (\"total reviews \",len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-22T01:48:05.163720Z",
     "start_time": "2022-04-22T01:48:04.983349Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(vocabulary=selected_words, lowercase=False)\n",
    "#corpus = ['This is the first document.','This is the second second document.']\n",
    "#print corpus\n",
    "selected_word_count = vectorizer.fit_transform(tip['text'].values.astype('U'))\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_array = selected_word_count.toarray()\n",
    "word_count_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_array.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(index=vectorizer.get_feature_names(), \\\n",
    "                    data=word_count_array.sum(axis=0)).rename(columns={0: 'Count'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.plot(kind='bar', stacked=False, figsize=[7,7], colormap='winter')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing For Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE Dummy Variables\n",
    "# Tokenizing/Vectorizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T18:16:46.247143Z",
     "start_time": "2022-04-21T18:16:46.242080Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions = clean_df['transactions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T18:17:05.950046Z",
     "start_time": "2022-04-21T18:17:05.767135Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use ast module to convert string objects into list\n",
    "transactions = transactions.apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T18:17:07.606777Z",
     "start_time": "2022-04-21T18:17:07.559656Z"
    }
   },
   "outputs": [],
   "source": [
    "# Change list into dicts\n",
    "dcts = transactions.apply(lambda x: {c: 1 for c in x})\n",
    "\n",
    "# Create new dataframe based on the list of dictionaries.\n",
    "ohe_df = pd.DataFrame(dcts.tolist()).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T18:17:08.934288Z",
     "start_time": "2022-04-21T18:17:08.921960Z"
    }
   },
   "outputs": [],
   "source": [
    "ohe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T18:17:11.339042Z",
     "start_time": "2022-04-21T18:17:11.283007Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate dummy variales with the full dataset\n",
    "df_1 = pd.concat([clean_df,ohe_df],axis=1)\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T14:37:04.983979Z",
     "start_time": "2022-04-21T14:37:04.700902Z"
    }
   },
   "outputs": [],
   "source": [
    "# Joining the reviews text to a corpus\n",
    "all_text = ' '.join(df.Reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create a TfidfVectorizer object with english stop words\n",
    "# and a maximum of 1500 features (to ensure that we can\n",
    "# train the model in a reasonable amount of time)\n",
    "vec = TfidfVectorizer(stop_words='english',\n",
    "                      max_features=1500)\n",
    "\n",
    "# create the TfIdf feature matrix from the raw text\n",
    "train_tfidf = vec.fit_transform(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a quick look at some of the features\n",
    "pd.DataFrame(data=train_tfidf.todense(), columns=vec.get_feature_names()).iloc[:5, 30:45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigrams\n",
    "\n",
    "# Bigrams\n",
    "\n",
    "# Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T03:06:56.222527Z",
     "start_time": "2022-04-21T02:13:31.541Z"
    }
   },
   "outputs": [],
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "def review_sentiment(review):\n",
    "    # Replace None with appropriate code\n",
    "    if review['rating'] >= 4:\n",
    "        print('positive')\n",
    "    elif review['rating'] <= 2:\n",
    "        print('negative')\n",
    "    else:\n",
    "        print('neutral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T21:38:52.000003Z",
     "start_time": "2022-04-27T21:38:51.723017Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T21:39:17.347708Z",
     "start_time": "2022-04-27T21:39:17.269747Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp =  spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model the data with out text first\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T23:59:09.386742Z",
     "start_time": "2022-04-26T23:59:08.408873Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    seas_feats_df, \n",
    "    seas_labels_df, \n",
    "    test_size =.25, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# get just the targets from the training labels\n",
    "train_targets = train_labels[['*', '**', '***']].astype(np.float64)\n",
    "\n",
    "# create a Linear regresion object\n",
    "ols = linear_model.LinearRegression()\n",
    "\n",
    "# fit that object on the training TfIdf matrix and target variables\n",
    "ols.fit(train_tfidf, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to produce the model's coefficients\n",
    "\n",
    "def eval_clf(model, X_test_tf,y_test,cmap='Reds',\n",
    "                            normalize='true',classes=['Unvaccinated', 'Vaccinated'],figsize=(10,4),\n",
    "                            X_train = None, y_train = None,):\n",
    "    \"\"\"Evaluates a scikit-learn binary classification model.\n",
    "\n",
    "    Args:\n",
    "        model ([type]): [description]\n",
    "        X_test_tf ([type]): [description]\n",
    "        y_test ([type]): [description]\n",
    "        cmap (str, optional): [description]. Defaults to 'Reds'.\n",
    "        normalize (str, optional): [description]. Defaults to 'true'.\n",
    "        classes ([type], optional): [description]. Defaults to None.\n",
    "        figsize (tuple, optional): [description]. Defaults to (8,4).\n",
    "        X_train ([type], optional): [description]. Defaults to None.\n",
    "        y_train ([type], optional): [description]. Defaults to None.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    y_hat_test = model.predict(X_test_tf)\n",
    "    print(metrics.classification_report(y_test, y_hat_test,target_names=classes))\n",
    "    \n",
    "\n",
    "    fig,ax = plt.subplots(ncols=2,figsize=figsize)\n",
    "    plt.grid(False)\n",
    "    plot_confusion_matrix(model, X_test_tf,y_test,cmap=cmap, \n",
    "                                  normalize=normalize,display_labels=classes,\n",
    "                                 ax=ax[0])\n",
    "    for a in ax:\n",
    "        a.grid(False)   \n",
    "        \n",
    "    curve = metrics.plot_roc_curve(model,X_test_tf,y_test,ax=ax[1])\n",
    "    curve.ax_.grid()\n",
    "    curve.ax_.plot([0,1],[0,1],ls=':')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    ## Add comparing Scores if X_train and y_train provided.\n",
    "    if (X_train is not None) & (y_train is not None):\n",
    "        print(f\"Training Score = {model.score(X_train,y_train):.2f}\")\n",
    "        print(f\"Test Score = {model.score(X_test_tf,y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating baseline classifier model\n",
    "\n",
    "base = DummyClassifier(strategy='stratified', random_state = 42)\n",
    "\n",
    "base.fit(X_train_df, y_train)\n",
    "\n",
    "eval_clf(base,X_test_tf,y_test,X_train=X_train_df,y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the names of the features\n",
    "feature_names = np.array(vec.get_feature_names())\n",
    "\n",
    "def get_top_features(features, model, level, limit, bottom=False):\n",
    "    \"\"\" Get the top (most likely to see violations) and bottom (least\n",
    "        likely to see violations) features for a given model.\n",
    "        \n",
    "        :param features: an array of the feature names\n",
    "        :param model: a fitted linear regression model\n",
    "        :param level: 0, 1, 2 for *, **, *** violation levels\n",
    "        :param limit: how many features to return\n",
    "        :param worst: if we want the bottom features rather than the top \n",
    "    \"\"\"\n",
    "    # sort order for the coefficients\n",
    "    sorted_coeffs = np.argsort(model.coef_[i])\n",
    "    \n",
    "    if bottom:\n",
    "        # get the features at the end of the sorted list\n",
    "        return features[sorted_coeffs[-1 * limit:]]\n",
    "    else:\n",
    "        # get the features at the beginning of the sorted list\n",
    "        return features[sorted_coeffs[:limit]]\n",
    "    \n",
    "# get the features that indicate we are most and least likely to see violations\n",
    "worst_feature_sets = [get_top_features(feature_names, ols, i, 100) for i in range(3)]\n",
    "best_feature_sets = [get_top_features(feature_names, ols, i, 100, bottom=True) for i in range(3)]\n",
    "\n",
    "# reduce the independent feature sets to just the ones\n",
    "# that we see in common across the per-level models (*, **, ***)\n",
    "worst = reduce(np.intersect1d, best_feature_sets)\n",
    "best = reduce(np.intersect1d, worst_feature_sets)\n",
    "\n",
    "# display as a pretty table\n",
    "html_fmt = \"<table><th>More Violations</th><th>Fewer Violations</th><tbody>{}</tbody></table>\"\n",
    "table_rows = [\"<tr><td>{}</td><td>{}</td></tr>\".format(w, b) for w, b in zip(worst, best)]\n",
    "table_body = \"\\n\".join(table_rows)\n",
    "display.HTML(html_fmt.format(table_body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways and Recommended Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Next Steps and Future Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "207px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
